{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# vim: set fileencoding=utf8 :\n",
    "#```\n",
    "\n",
    "#!pip install -U boto3 retrying\n",
    "#!export AWS_DEFAULT_PROFILE=test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Athena to extract features on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The dataset we are working with contains 55M records, making its handling too heavy for a single machine.</p>\n",
    "<p>Using a distributed computing engine like&nbsp;<a href=\"https://aws.amazon.com/athena/\">AWS Athena</a>&nbsp;will enable you to extract features and save data efficiently.&nbsp;</p>\n",
    "<p>In order to work on the data, we upload it to S3, and than partition it using AWS Glue. Partitioning is critical to make Athena run efficiently. For examples on how to use Glue, go&nbsp;<a href=\"https://github.com/doitintl/aws-glue-workshop\">HERE</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the data partitioned (say, by year and month), run the following Athena query to extract the following features&nbsp;</p>\n",
    "<p>After extracting features, partition the query results using Glue (again)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATABASE IF NOT EXISTS taxinyc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE EXTERNAL TABLE IF NOT EXISTS taxinyc.raw_data (\n",
    "               key VARCHAR(255),\n",
    "               fare_amount FLOAT,\n",
    "               pickup_datetime VARCHAR(255),\n",
    "               pickup_longitude FLOAT,\n",
    "               pickup_latitude FLOAT,\n",
    "               dropoff_longitude FLOAT,\n",
    "               dropoff_latitude FLOAT,\n",
    "               passenger_count INT\n",
    "               )\n",
    "               ROW FORMAT DELIMITED\n",
    "               FIELDS TERMINATED BY \",\"\n",
    "               LINES TERMINATED BY \"\\n\"\n",
    "               LOCATION 's3://aws-worskhop-data/taxi-nyc'\n",
    "               TBLPROPERTIES (\n",
    "               'skip.header.line.count' = '1'\n",
    "               );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT * FROM \"taxinyc\".\"raw_data\" limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL = '''\n",
    "WITH \n",
    "    dataset AS \n",
    "    (SELECT CAST (pickup_datetime AS TIMESTAMP WITH time zone) AT TIME ZONE 'America/New_York' AS est, \n",
    "                  ST_POINT(pickup_longitude,pickup_latitude) pickup_point,\n",
    "                  ST_POINT(dropoff_longitude,dropoff_latitude) dropoff_point,\n",
    "                  to_unixtime( CAST (pickup_datetime AS TIMESTAMP WITH time zone) AT TIME ZONE 'America/New_York') AS                     epoch,\n",
    "                  24*60*60 as seconds_in_day,\n",
    "                  *\n",
    "      FROM train_v3),\n",
    "    \n",
    "    airports AS (SELECT \n",
    "                  kv['LaGuardia'] AS LaGuardia,\n",
    "                  kv['Downtown Manhattan/Wall St. Heliport'] AS Manhattan,\n",
    "                  kv['John F Kennedy Intl'] AS JFK\n",
    "    FROM (SELECT map_agg(name, point_location) kv\n",
    "        FROM \n",
    "            (SELECT name,\n",
    "         ST_POINT(longitude,\n",
    "         latitude) point_location\n",
    "            FROM usa_airports\n",
    "            WHERE city = 'New York' )\n",
    "            ))\n",
    "        SELECT \n",
    "        \n",
    "        -- Target\n",
    "         fare_amount,\n",
    "         \n",
    "         -- time features\n",
    "         day(est) day,\n",
    "         day_of_week(est) dayofweek ,\n",
    "         year(est) year ,\n",
    "         month(est) month ,\n",
    "         day_of_month(est) dayofmonth ,\n",
    "         hour(est) hour ,\n",
    "         minute(est) minute ,\n",
    "         \n",
    "         -- cyclclical variables\n",
    "         sin(2*pi()*epoch/seconds_in_day) sin_day,\n",
    "         cos(2*pi()*epoch/seconds_in_day) cos_day,\n",
    "         sin(2*pi()*epoch/(seconds_in_day*7)) sin_week,\n",
    "         cos(2*pi()*epoch/(seconds_in_day*7)) cos_week,\n",
    "         \n",
    "         \n",
    "         -- Distance features\n",
    "         pickup_longitude - dropoff_longitude diff_longitude,\n",
    "         pickup_latitude - dropoff_latitude diff_latitude,\n",
    "         ST_Distance(pickup_point, dropoff_point) dist,\n",
    "         \n",
    "         -- Airports features\n",
    "         ST_DISTANCE(airports.LaGuardia, dropoff_point) dropoff_laguardia,\n",
    "         ST_DISTANCE(airports.LaGuardia, pickup_point ) pickup_laguardia,\n",
    "         ST_DISTANCE(airports.JFK, dropoff_point) dropoff_JFK,\n",
    "         ST_DISTANCE(airports.JFK, pickup_point) pickup_JFK,\n",
    "         ST_DISTANCE(airports.Manhattan, dropoff_point) dropoff_manhattan,\n",
    "         ST_DISTANCE(airports.Manhattan, pickup_point) pickup_manhattan,\n",
    "         \n",
    "         -- Raw features\n",
    "         pickup_longitude,\n",
    "         pickup_latitude,\n",
    "         dropoff_longitude,\n",
    "         dropoff_latitude,\n",
    "         passenger_count\n",
    "         \n",
    "    FROM dataset, airports\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH \r\n",
      "    dataset AS \r\n",
      "    (SELECT CAST (pickup_datetime AS TIMESTAMP WITH time zone) AT TIME ZONE 'America/New_York' AS est, \r\n",
      "                  ST_POINT(pickup_longitude,pickup_latitude) pickup_point,\r\n",
      "                  ST_POINT(dropoff_longitude,dropoff_latitude) dropoff_point,\r\n",
      "                  to_unixtime( CAST (pickup_datetime AS TIMESTAMP WITH time zone) AT TIME ZONE 'America/New_York') AS                     epoch,\r\n",
      "                  24*60*60 as seconds_in_day,\r\n",
      "                  *\r\n",
      "     FROM raw_data)\r\n",
      "    \r\n",
      "     SELECT\r\n",
      "     \r\n",
      "        -- Target\r\n",
      "        fare_amount,\r\n",
      "        \r\n",
      "        -- time features\r\n",
      "        day(est) day,\r\n",
      "        day_of_week(est) dayofweek ,\r\n",
      "        year(est) year ,\r\n",
      "        month(est) month ,\r\n",
      "        day_of_month(est) dayofmonth ,\r\n",
      "        hour(est) hour ,\r\n",
      "        minute(est) minute ,\r\n",
      "         \r\n",
      "        -- cyclclical variables\r\n",
      "        sin(2*pi()*epoch/seconds_in_day) sin_day,\r\n",
      "        cos(2*pi()*epoch/seconds_in_day) cos_day,\r\n",
      "        sin(2*pi()*epoch/(seconds_in_day*7)) sin_week,\r\n",
      "        cos(2*pi()*epoch/(seconds_in_day*7)) cos_week,\r\n",
      "     \r\n",
      "        -- Raw features\r\n",
      "        pickup_datetime,\r\n",
      "        pickup_longitude,\r\n",
      "        pickup_latitude,\r\n",
      "        dropoff_longitude,\r\n",
      "        dropoff_latitude,\r\n",
      "        passenger_count\r\n",
      "         \r\n",
      "    FROM dataset\r\n"
     ]
    }
   ],
   "source": [
    "!cat athena_taxi_raw.sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athena_taxi_raw.sql\r\n"
     ]
    }
   ],
   "source": [
    "!python athena.py athena_taxi_raw.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo.sql\r\n"
     ]
    }
   ],
   "source": [
    "!python athena.py foo.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: athena_taxi_raw.sql.csv (deflated 64%)\n"
     ]
    }
   ],
   "source": [
    "!zip taxinyc_train.csv.zip athena_taxi_raw.sql.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!split taxinyc_train.csv.zip -b 300M ZIPCHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm athena_taxi_raw.sql.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm taxinyc_train.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_sagemaker = !ls -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['athena.log',\n",
       " 'athena.py',\n",
       " 'athena_taxi_raw.sql',\n",
       " 'athena_taxi_raw.sql.log',\n",
       " 'carparts49',\n",
       " 'foo.sql',\n",
       " 'foo.sql.csv',\n",
       " 'foo.sql.log',\n",
       " 'lost+found',\n",
       " 'run_athena_query.ipynb',\n",
       " 'taxi_fare_prediction.ipynb',\n",
       " 'taxi_fare_prediction_original.ipynb',\n",
       " 'train_small_2014.csv',\n",
       " 'validate_small_2015.csv',\n",
       " 'ZIPCHUNKSaa',\n",
       " 'ZIPCHUNKSab',\n",
       " 'ZIPCHUNKSac',\n",
       " 'ZIPCHUNKSad',\n",
       " 'ZIPCHUNKSae',\n",
       " 'ZIPCHUNKSaf',\n",
       " 'ZIPCHUNKSag',\n",
       " 'ZIPCHUNKSah',\n",
       " 'ZIPCHUNKSai',\n",
       " 'ZIPCHUNKSaj',\n",
       " 'ZIPCHUNKSak',\n",
       " 'ZIPCHUNKSal',\n",
       " 'ZIPCHUNKSam']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "athena.log  # program log\n",
    "athena.py   # main program\n",
    "foo.sql     # query execution result\n",
    "foo.sql.csv # sql output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs.core import S3FileSystem\n",
    "import os\n",
    "\n",
    "s3 = S3FileSystem(anon=False)\n",
    "bucket = 'aws-worskhop-data'\n",
    "\n",
    "file_list = ls_sagemaker\n",
    "subs = 'ZIPCHUNKS'\n",
    "ZIPCHUNKS_list = [i for i in file_list if subs in i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZIPCHUNKSaa',\n",
       " 'ZIPCHUNKSab',\n",
       " 'ZIPCHUNKSac',\n",
       " 'ZIPCHUNKSad',\n",
       " 'ZIPCHUNKSae',\n",
       " 'ZIPCHUNKSaf',\n",
       " 'ZIPCHUNKSag',\n",
       " 'ZIPCHUNKSah',\n",
       " 'ZIPCHUNKSai',\n",
       " 'ZIPCHUNKSaj',\n",
       " 'ZIPCHUNKSak',\n",
       " 'ZIPCHUNKSal',\n",
       " 'ZIPCHUNKSam']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ZIPCHUNKS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'aws-worskhop-data'\n",
    "file_path = 'ZIPCHUNKS/' \n",
    "s3_path = os.path.join('s3://', bucket, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://aws-worskhop-data/ZIPCHUNKS/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ZIPCHUNKSaa\n",
      "upload: ./ZIPCHUNKSaa to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSaa\n",
      "./ZIPCHUNKSab\n",
      "upload: ./ZIPCHUNKSab to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSab\n",
      "./ZIPCHUNKSac\n",
      "upload: ./ZIPCHUNKSac to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSac\n",
      "./ZIPCHUNKSad\n",
      "upload: ./ZIPCHUNKSad to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSad\n",
      "./ZIPCHUNKSae\n",
      "upload: ./ZIPCHUNKSae to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSae\n",
      "./ZIPCHUNKSaf\n",
      "upload: ./ZIPCHUNKSaf to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSaf\n",
      "./ZIPCHUNKSag\n",
      "upload: ./ZIPCHUNKSag to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSag\n",
      "./ZIPCHUNKSah\n",
      "upload: ./ZIPCHUNKSah to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSah\n",
      "./ZIPCHUNKSai\n",
      "upload: ./ZIPCHUNKSai to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSai\n",
      "./ZIPCHUNKSaj\n",
      "upload: ./ZIPCHUNKSaj to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSaj\n",
      "./ZIPCHUNKSak\n",
      "upload: ./ZIPCHUNKSak to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSak\n",
      "./ZIPCHUNKSal\n",
      "upload: ./ZIPCHUNKSal to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSal\n",
      "./ZIPCHUNKSam\n",
      "upload: ./ZIPCHUNKSam to s3://aws-worskhop-data/ZIPCHUNKS/ZIPCHUNKSam\n"
     ]
    }
   ],
   "source": [
    "for file in ZIPCHUNKS_list: \n",
    "    local_path = os.path.join('./', file)\n",
    "    print (local_path)\n",
    "    !aws s3 cp $local_path $s3_path \n",
    "    !rm $local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
